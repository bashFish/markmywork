{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmw.parse import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThis is indeed a test'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.organization = os.getenv(\"OPENAI_ORGANIZATION\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "response = openai.Completion.create(model=\"text-davinci-003\", prompt=\"Say this is a test\", temperature=0, max_tokens=7)\n",
    "response['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_explanation = parse_question_explanations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parsed_essays = parse_essays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "marked_essays, filtered_marked_essays, all_questions = parse_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 76)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_parsed_essays), len(filtered_marked_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = \"text-davinci-003\"\n",
    "#model = \"text-ada-001\"\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "#models simple/ cheap/ fast to advanced:\n",
    "#model = \"ada\"\n",
    "#model = \"babbage\"\n",
    "#model = \"curie\"\n",
    "#model = \"davinci\"\n",
    "\n",
    "#model = \"ada:ft-mark-my-words-2023-05-01-11-44-26\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tpl = 'You are a caring teacher who wants to help their students succeed by understanding their mistakes and how to improve. Given is a student essay and a graded remark to a question. Grades can be numbers between 0 (lowest, worst) and 3 (highest, best). Give a possible reason and detailed explanation for the graded remark along with a reference from the Essay. Use direct form to answer the student. Start with a positive feedback.\\n\\n###\\n\\n'\n",
    "prompt_tpl += 'Essay:\\n{essay}\\n\\n###\\n\\n'\n",
    "prompt_tpl += 'Question:\\n{explanation}\\n{question}\\n\\n###\\n\\n'\n",
    "prompt_tpl += 'Grade: {remark}\\n\\n###\\n\\n'\n",
    "prompt_tpl += 'Answer:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_citation_prompts(all_parsed_essays, filtered_marked_essays, questions_explanation):\n",
    "\n",
    "    all_prompts = []\n",
    "    for i,essay_array in tqdm.tqdm(enumerate(all_parsed_essays)):\n",
    "\n",
    "        essay_id = int(re.findall(r'\\d+', essay_array[0])[-1])\n",
    "        cur_row = filtered_marked_essays[filtered_marked_essays['essay id'] == essay_id]\n",
    "        if len(cur_row) != 1:\n",
    "            continue\n",
    "\n",
    "        for j,k in questions_explanation.iterrows():\n",
    "            question = k.question\n",
    "            explanation = k.explanation\n",
    "            grade = int(cur_row.iloc[0,j+1])\n",
    "            prompt = prompt_tpl.format(essay=essay_array[3][:-1], #implicit newline\n",
    "                                    question=question.strip(),\n",
    "                                    explanation=explanation.strip(),\n",
    "                                    remark=grade)\n",
    "\n",
    "            all_prompts.append((i, j, prompt, grade))\n",
    "    return all_prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [00:00, 486.82it/s]\n"
     ]
    }
   ],
   "source": [
    "all_prompts = assemble_citation_prompts(all_parsed_essays, filtered_marked_essays, questions_explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [06:46<22:23, 16.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception, retrying 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [29:51<00:00, 17.91s/it]\n"
     ]
    }
   ],
   "source": [
    "all_answers = []\n",
    "for prompt_array in tqdm.tqdm(all_prompts[:100]): #[:LIMIT_PROMPTS*len(questions_explanation)]):\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            i, j, prompt, annot = prompt_array\n",
    "            all_splits = prompt.split('\\n\\n###\\n\\n')\n",
    "            message = '\\n\\n###\\n\\n'.join(all_splits[1:])\n",
    "            response = openai.ChatCompletion.create(model=model, messages=[\n",
    "                {'role': 'system', 'content': all_splits[0]},\n",
    "                {'role': 'user', 'content': message}],\n",
    "                                                    temperature=0, max_tokens=250)\n",
    "            all_answers.append((i, j, prompt, response, annot))\n",
    "            \n",
    "            time.sleep(.5)\n",
    "            break\n",
    "        except:\n",
    "            print(f\"exception, retrying {i}\")\n",
    "            time.sleep(2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results('data/evaluations/run_8_chatgpt_comments', all_parsed_essays, all_questions, all_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great job on writing a narrative essay! Your opening sentence definitely grabs the reader\\'s attention by creating a sense of mystery and intrigue. However, there are a few areas where you can improve to make it more compelling. Your use of grammar and spelling needs some work, as there are several errors throughout the essay. Additionally, try to avoid going off-topic and stay focused on the main point of your essay. Keep up the good work and keep practicing your writing skills! \\n\\nRegarding the graded remark, you received a grade of 1 for the opening sentence. While it does create a sense of mystery, there are some areas where it could be improved to make it more compelling. For example, you could try to use more descriptive language to paint a vivid picture in the reader\\'s mind. Additionally, try to avoid using slang or informal language, as it can detract from the overall quality of your writing. Keep practicing and experimenting with different techniques to improve your writing skills. \\n\\nReference: \"It felt like I was walking forever, i didn\\'t even know were i was i just knew i was somewhere. A creepy aliway to be exazaced.\"'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_answers[0][3]['choices'][0]['message']['content']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
