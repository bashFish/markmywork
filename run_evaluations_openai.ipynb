{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmw.parse import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThis is indeed a test'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.organization = os.getenv(\"OPENAI_ORGANIZATION\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "response = openai.Completion.create(model=\"text-davinci-003\", prompt=\"Say this is a test\", temperature=0, max_tokens=7)\n",
    "response['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parsed_essays = parse_essays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "marked_essays, filtered_marked_essays, all_questions = parse_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 76)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_parsed_essays), len(filtered_marked_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = \"text-davinci-003\"\n",
    "model = \"ada:ft-mark-my-words-2023-05-01-11-44-26\" #ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT_ESSAYS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:09,  9.18s/it]\n"
     ]
    }
   ],
   "source": [
    "all_answers = []\n",
    "for i,essay_array in tqdm.tqdm(enumerate(all_parsed_essays[:LIMIT_ESSAYS])):\n",
    "    \n",
    "    essay_id = int(re.findall(r'\\d+', essay_array[0])[-1])\n",
    "    cur_row = filtered_marked_essays[filtered_marked_essays['essay id'] == essay_id]\n",
    "    if len(cur_row) != 1:\n",
    "        continue\n",
    "\n",
    "    for j,k in enumerate(all_questions):\n",
    "        \n",
    "        prompt = assemble_prompt(essay_array[3][:-1], #implicit newline\n",
    "                                 str(k).strip())\n",
    "\n",
    "        response = openai.Completion.create(model=model, prompt=prompt, temperature=0, max_tokens=3)\n",
    "        all_answers.append((i, j, prompt, response, int(cur_row[k])))\n",
    "        \n",
    "        time.sleep(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " 0,\n",
       " 'Given is a student essay and a question to the essay. Please rate the Question with a number between 0 and 3, where 0 is \"not at all satisfied\" and 3 is \"very satisfied.\".\\n\\nEssay: \"\"\"\\nThe Cage\\nIt felt like I was walking forever, i didn\\'t even know were i was i just knew i was somewhere. A creepy aliway to be exazaced. To be honest i would rather\\nbe at school than this nightmare well not to brag but i was not scared in the slightited, anyway desided just to keep walking to see what was down this\\naliway, just as i was almost there thunder shot right though the sky it was like someone had a shotgun shot a bullet right next to my ear jezz i was not\\nprepared for that, where there is thunder there is rain and i was not gonna get my new guchi shoes wet so i bulted to the end of the crusty aliway,\\ni saw... well i didn\\'t see anything other than cages, well i guess it was just a dream cause i don\\'t own\\nguchi shoes. OK enough of me being broke I gotta get to school. Time forwords and i\\'m at school barely and my best friend walks towards me talking about\\nhow her dad didn\\'t buy her the new iphone 507989428 which is like a billion bucks while i\\'m sitting here trying to listen to her complaints that no one\\ncares about while i don\\'t even have a dad\"\"\"\\nQuestion: \"\"\"\\nHas the student written a compelling opening?\"\"\"\\nAnswer:',\n",
       " <OpenAIObject text_completion id=cmpl-7BO0wzrUucZjrVG2DlOmt1DmJjSKT at 0x7fce986d6ea0> JSON: {\n",
       "   \"choices\": [\n",
       "     {\n",
       "       \"finish_reason\": \"length\",\n",
       "       \"index\": 0,\n",
       "       \"logprobs\": null,\n",
       "       \"text\": \" 0\\nAnswer\"\n",
       "     }\n",
       "   ],\n",
       "   \"created\": 1682948274,\n",
       "   \"id\": \"cmpl-7BO0wzrUucZjrVG2DlOmt1DmJjSKT\",\n",
       "   \"model\": \"ada:ft-mark-my-words-2023-05-01-11-44-26\",\n",
       "   \"object\": \"text_completion\",\n",
       "   \"usage\": {\n",
       "     \"completion_tokens\": 3,\n",
       "     \"prompt_tokens\": 330,\n",
       "     \"total_tokens\": 333\n",
       "   }\n",
       " },\n",
       " 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results('data/evaluations/run_4', all_parsed_essays, all_questions, all_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,all_answers_loaded = load_results('data/evaluations/run_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, ' 0\\nAnswer'),\n",
       " (0, 1, ' 0\\n\\n'),\n",
       " (0, 2, ' 0\\n\\n'),\n",
       " (0, 3, ' 1\\nQuestion'),\n",
       " (0, 4, ' 1\\nQuestion'),\n",
       " (0, 5, ' 0\\nQuestion'),\n",
       " (0, 6, ' 0\\nQuestion'),\n",
       " (0, 7, ' 0\\n\\n'),\n",
       " (0, 8, ' 0\\n\\n'),\n",
       " (0, 9, ' 0\\n\\n'),\n",
       " (0, 10, ' 0\\nQuestion'),\n",
       " (0, 11, ' 0\\n\\n'),\n",
       " (0, 12, ' 0\\nAnswer')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(a[0], a[1], a[3]['choices'][0]['text']) for a in all_answers_loaded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, ' 0\\nAnswer'),\n",
       " (0, 1, ' 0\\n\\n'),\n",
       " (0, 2, ' 0\\n\\n'),\n",
       " (0, 3, ' 1\\nQuestion'),\n",
       " (0, 4, ' 1\\nQuestion'),\n",
       " (0, 5, ' 0\\nQuestion'),\n",
       " (0, 6, ' 0\\nQuestion'),\n",
       " (0, 7, ' 0\\n\\n'),\n",
       " (0, 8, ' 0\\n\\n'),\n",
       " (0, 9, ' 0\\n\\n'),\n",
       " (0, 10, ' 0\\nQuestion'),\n",
       " (0, 11, ' 0\\n\\n'),\n",
       " (0, 12, ' 0\\nAnswer')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(a[0],a[1],a[3]['choices'][0]['text']) for a in all_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
